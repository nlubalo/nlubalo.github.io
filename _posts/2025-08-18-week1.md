---
layout: post-product-queen
title:  "Building the Foundation: From Text to Embeddings"
image: assets/images/home.jpg
description: "What happens under the hood when you type the words I love AI into ChatGPT?
    Most people think the model directly understands the text as is, the reality however is far more complex. The model lacks any true understanding of the world and operates purely on statistical patterns."
categories: [projects]
---

<p>
When we talk to each other, we use words, tone, and context. But computers don’t “see” words, they see numbers. The first challenge in Natural Language Processing (NLP) is teaching machines how to take a sentence like.
</p>
In this post, we’ll start from scratch (rolling our own encoding system), and then graduate into the modern world of Transformers using Hugging Face.
```python
"I love working with data and building AI models."
```
…and break it down into a form a machine can work with.
<p>
   Our first step is tokenization: breaking the sentence into smaller parts (tokens). For now, let’s use a very simple tokenizer that just splits on spaces:
</p>
```python
sentence = "I love working with data and building AI models."
tokens = sentence.lower().replace(".", "").split()
print(tokens)
```
Output:
```
['i', 'love', 'working', 'with', 'data', 'and', 'building', 'ai', 'models']
```
We’ve turned a sentence into a clean list of words called tokens. But machines still don’t understand these words. They need a way to represent them numerically.
---

### Step 2: Assign IDs — numbers behind the words
Next, we build a tiny vocabulary and map each word to a unique integer:
```python
vocab = {word: i for i, word in enumerate(tokens)}
print(vocab)
```
Output:
```
{'i': 0, 'love': 1, 'working': 2, 'with': 3, 'data': 4, 'and': 5, 'building': 6, 'ai': 7, 'models': 8}
```
<p>
Now our sentence becomes just a sequence of IDs:
</p>
```python
ids = [vocab[word] for word in tokens]
print(ids)
```
Output:
```
[0, 1, 2, 3, 4, 5, 6, 7, 8]
```

## Step 3: Embeddings — giving words meaning
But integers alone don’t tell us much — “data” (4) and “models” (8) look unrelated. That’s why we use embeddings: vectors in a high-dimensional space that capture meaning.

Let’s make a toy embedding matrix:
```python
import numpy as np

embedding_dim = 5  # keep it small for illustration
embedding_matrix = np.random.rand(len(vocab), embedding_dim)

# Look up embeddings for our sentence
sentence_embeddings = np.array([embedding_matrix[word_id] for word_id in encoded])
print(sentence_embeddings)
```
Now each word is represented by a 5D vector (in real models, dimensions are much higher, like 300, 512, or 768).

Output:
```
[[0.12 0.88 0.45 0.67 0.21]
 [0.76 0.34 0.11 0.98 0.44]
 ...
 [0.65 0.73 0.29 0.55 0.19]]
```

Here we create an initial random matrix of (number of words * dimension_size). Dimension size is like "features" of meaning like for example to capture the positive and negative context we can use 2D, to capture a more nuanced relationships (e.g., tense, gender, context, topic) we can use 5D.. The selection of dimension size is mostly driven by data size , vocabulary size, compute poers and how much information about a word you want to capture.
- This matrix is started as random because it hasn't been trained yet. In real models like the GPTs, these embeddings get trained by a process known as backpropagation so that similar words end up with similar vectors. E,g King and Queen are close together
- At the start, they are just random placeholders that the model learns to adjust during training


## Part 2: Modern Encodings with Transformers
Our toy setup is useful for learning, but real-world NLP uses subword tokenization and pre-trained embeddings from massive models.

Let’s see how Hugging Face makes this easy.
Use a tokenizer from BERT
```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

sentence = "I love working with data and building AI models."
inputs = tokenizer(sentence, return_tensors="pt")

print(inputs["input_ids"])
print(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0]))
```
Unlike our space-split tokenizer, BERT’s tokenizer breaks words into subwords like ["i", "love", "working", "with", "data", "and", "building", "ai", "models"] or smaller chunks if it sees unknown words

Extract embeddings from BER
```python
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state
    print(embeddings.shape)  # (batch_size, sequence_length, hidden_size)
```
Output:
```
torch.Size([1, 9, 768])
```
This gives us a tensor of shape (1, 9, 768) — 9 tokens, each with a 768-dimensional embedding vector.

Now each token has a rich 768-dimensional embedding learned from billions of words.

## Why This Journey Matters

- In Part 1, we hand-crafted encodings to learn the basics. We saw the steps: tokenization → IDs → embeddings.

- In Part 2, we saw how modern models like BERT use advanced tokenization and provide pre-trained embeddings that capture meaning far better.

This transition is the story of NLP itself:

    - from simple word-to-ID tricks → to embeddings → to massive pre-trained models.

And once we have embeddings, we can use them for similarity search, clustering, classification, and powering generative AI.

---